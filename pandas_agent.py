from langchain_ollama import OllamaLLM
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END, StateGraph, MessagesState
from langgraph.prebuilt import ToolNode

# Ferramenta para interagir com DataFrames do pandas
import pandas as pd
import uuid  # Para gerar um UUID √∫nico para o thread_id

# Carrega o arquivo CSV (ajuste o caminho do arquivo)
df = pd.read_csv("C:/Users/pedro/OneDrive/Documentos/basesdedefeitos/jirabugs.csv")

@tool
def analyze_epics(query: str):
    """
    Ferramenta para analisar a frequ√™ncia dos √©picos no DataFrame pandas.
    """
    if "frequ√™ncia de √©picos" in query.lower():
        # Conta a frequ√™ncia de cada √©pico
        epic_counts = df["Epic Link"].value_counts()
        return f"A frequ√™ncia de ocorr√™ncia por √©pico √©: {epic_counts.to_dict()}"
    elif "√©pico mais comum" in query.lower():
        # Identifica o √©pico mais frequente
        most_common_epic = df["Epic Link"].value_counts().idxmax()
        count = df["Epic Link"].value_counts().max()
        return f"O √©pico mais comum √© '{most_common_epic}' com {count} ocorr√™ncias."
    else:
        return "Desculpe, n√£o consegui entender a pergunta. Tente ser mais espec√≠fico sobre a an√°lise de √©picos."

# Lista de ferramentas dispon√≠veis
tools = [analyze_epics]

# Define o modelo da LLM (OllamaLLM)
llm = OllamaLLM(model="llama2")

# Integra ferramentas no fluxo (usando ToolNode)
tool_node = ToolNode(tools)

# Fun√ß√£o para decidir o pr√≥ximo passo no fluxo
def should_continue(state: MessagesState):
    """
    Decide se o fluxo deve continuar ou terminar, com base na resposta.
    """
    messages = state["messages"]
    last_message = messages[-1]
    if last_message.tool_calls:
        return "tools"  # Rota para usar as ferramentas
    return END  # Termina o fluxo

# Fun√ß√£o que chama a LLM para responder
def call_model(state: MessagesState):
    """
    Chama o modelo LLM para gerar respostas.
    """
    messages = state["messages"]
    response = llm.invoke(messages)
    return {"messages": [response]}

# Cria o gr√°fico do LangGraph para fluxo do agente
workflow = StateGraph(MessagesState)

# Adiciona os n√≥s ao gr√°fico
workflow.add_node("agent", call_model)  # N√≥ principal da LLM
workflow.add_node("tools", tool_node)  # N√≥ das ferramentas

# Define o ponto de entrada (primeiro n√≥ a ser executado)
workflow.add_edge(START, "agent")

# Adiciona condi√ß√µes para alternar entre n√≥s
workflow.add_conditional_edges("agent", should_continue)
workflow.add_edge("tools", "agent")

# Inicializa a mem√≥ria para salvar o estado
checkpointer = MemorySaver(thread_id=str(uuid.uuid4()))  # Adicionando thread_id gerado dinamicamente

# Compila o fluxo para ser usado
app = workflow.compile(checkpointer=checkpointer)

# Executa o fluxo
if __name__ == "__main__":
    print("‚ùì Pergunta: Qual √© a frequ√™ncia de ocorr√™ncia por √©pico?")
    initial_state = {"messages": [HumanMessage(content="Qual √© a frequ√™ncia de ocorr√™ncia por √©pico?")]}
    final_state = app.invoke(initial_state)
    print(f"ü§ñ Resposta: {final_state['messages'][-1].content}")
